{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e1795cffaafa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import PIL\n",
    "import numpy as np\n",
    "import math\n",
    "from xml.etree import ElementTree\n",
    "from contextlib import contextmanager\n",
    "import threading\n",
    "from google.colab import output\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install Go\n",
    "\n",
    "!add-apt-repository ppa:longsleep/golang-backports -y\n",
    "!apt update\n",
    "!apt install golang-go\n",
    "%env GOPATH=/root/go\n",
    "!go get -u github.com/gopherdata/gophernotes\n",
    "!cp ~/go/bin/gophernotes /usr/bin/\n",
    "!mkdir /usr/local/share/jupyter/kernels/gophernotes\n",
    "!cp ~/go/src/github.com/gopherdata/gophernotes/kernel/* \\\n",
    "       /usr/local/share/jupyter/kernels/gophernotes\n",
    "\n",
    "# then refresh notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install AuGoment\n",
    "! git clone https://github.com/lootag/ImageAuGomentationCLI.git\n",
    "os.chdir('ImageAuGomentationCLI' + os.sep + 'src')\n",
    "! make\n",
    "os.chdir('/content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mean_average_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/guglielmoG/windspeed.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from windspeed.utils import *\n",
    "\n",
    "repo = 'windspeed'\n",
    "detection_classes = ['flag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO_DELETE\n",
    "Possiamo mettere legacy code che non ha senso stia da altre parti ma che potrebbe essere utile in futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to rename pictures\n",
    "with cwd('windspeed', 'flags'):\n",
    "    # get all annots\n",
    "    ann = set(os.listdir('Annotations'))\n",
    "    c = 0\n",
    "    for f in os.listdir('images'):\n",
    "        img_name, ext = os.path.splitext(f)\n",
    "        if os.path.exists(join_path('Annotations', img_name+'.xml')):\n",
    "            os.rename(join_path('Annotations', img_name+'.xml'), join_path('Annotations', str(c) + 'flag.xml'))\n",
    "            os.rename(join_path('images', f), join_path('images', str(c)+'flag'+ext))\n",
    "            ann.remove(img_name+'.xml')\n",
    "        else:\n",
    "            os.rename(join_path('images', f), f)\n",
    "            print(f)\n",
    "        c += 1\n",
    "            \n",
    "    print('No images for ', ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to debug augoment, identify images for which creates problems\n",
    "with cwd(repo):\n",
    "    ! git checkout .\n",
    "    ! git clean -fd\n",
    "\n",
    "with cwd(repo, 'data', 'flags'):\n",
    "    os.mkdir('temp')\n",
    "    os.mkdir(join_path('temp', 'Images'))\n",
    "    os.mkdir(join_path('temp', 'Annotations'))\n",
    "    c = 0\n",
    "    for f in os.listdir('Images'):\n",
    "        img_name,_ = os.path.splitext(f)\n",
    "        os.rename(join_path('Annotations', img_name+'.xml'), join_path('temp', 'Annotations', img_name+'.xml'))\n",
    "        os.rename(join_path('Images', f), join_path('temp', 'Images', f))\n",
    "        try:\n",
    "            with cwd('temp'):\n",
    "                ! augoment -exclusion_threshold=1 -batch_size=1 -blur=2 >good 2>bad\n",
    "                ! echo {f} > boh\n",
    "                ! grep panic bad >> boh\n",
    "                tt = ! grep panic bad\n",
    "                if len(tt) != 0:\n",
    "                    ! cat boh >> out_good\n",
    "                    ! echo \"\" >> out_good\n",
    "        except Exception as e:\n",
    "            print(f, str(e))\n",
    "        finally:\n",
    "            os.remove(join_path('temp', 'Annotations', img_name+'.xml'))\n",
    "            os.remove(join_path('temp', 'Images', f))\n",
    "        print(c)\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END_TO_DELETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flag Detection\n",
    "\n",
    "Folder structure:  \n",
    "-train  \n",
    "-validation  \n",
    "-test  \n",
    "\n",
    "In each folder one can find images and annotations, separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folder Structure Creation\n",
    "Divide in training, validation and test set, with approximately 75%, 15% and 10% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3456)\n",
    "\n",
    "def make_augoment_dir(name):\n",
    "    os.mkdir(name)\n",
    "    os.mkdir(join_path(name, 'Images'))\n",
    "    os.mkdir(join_path(name, 'Annotations'))\n",
    "    \n",
    "with cwd(repo, 'data', 'flags'):\n",
    "    make_augoment_dir('train')\n",
    "    make_augoment_dir('valid')\n",
    "    make_augoment_dir('test')\n",
    "    imgs = os.listdir('Images')\n",
    "    n = len(imgs)\n",
    "    #use random indexes to shuffle the images\n",
    "    idx = np.arange(n)\n",
    "    np.random.shuffle(idx)\n",
    "    for i in range(n):\n",
    "        img = imgs[idx[i]]\n",
    "        img_name, _ = os.path.splitext(img)\n",
    "        if i < n * 0.75:\n",
    "            os.rename(join_path('Images', img), join_path('train', 'Images', img))\n",
    "            os.rename(join_path('Annotations', img_name + '.xml'), join_path('train', 'Annotations', img_name + '.xml'))\n",
    "        elif i < n * 0.9:\n",
    "            os.rename(join_path('Images', img), join_path('valid', 'Images', img))\n",
    "            os.rename(join_path('Annotations', img_name + '.xml'), join_path('valid', 'Annotations', img_name + '.xml'))\n",
    "        else:\n",
    "            os.rename(join_path('Images', img), join_path('test', 'Images', img))\n",
    "            os.rename(join_path('Annotations', img_name + '.xml'), join_path('test', 'Annotations', img_name + '.xml'))\n",
    "    os.rmdir('Images')\n",
    "    os.rmdir('Annotations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "NB: if one would like to obtain different blur values for a given image (i.e. produce say 3 new blurred images out of every image, with different blur values), they would need to rename previosly created blurred images, because they are overwritten.\n",
    "\n",
    "Convert images to jpg for augoment to work properly, then run augoment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_jpg(join_path(repo,'data','flags','train', 'Images'))\n",
    "convert_to_jpg(join_path(repo,'data','flags','valid', 'Images'))\n",
    "convert_to_jpg(join_path(repo,'data','flags','test', 'Images'))\n",
    "\n",
    "with cwd(join_path(repo,'data','flags','train')):\n",
    "    ! augoment -blur=2\n",
    "    \n",
    "with cwd(join_path(repo,'data','flags','valid')):\n",
    "    ! augoment -blur=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO Preprocessing\n",
    "\n",
    "Clone the repo, and compile code with CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/pjreddie/darknet.git\n",
    "\n",
    "with cwd('darknet'):\n",
    "    ! sed -i 's/GPU=0/GPU=1/' Makefile\n",
    "    ! sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n",
    "    ! make\n",
    "    \n",
    "#copy needed files\n",
    "os.rename(join_path('darknet', 'darknet'), join_path(repo, 'darknet'))\n",
    "os.mkdir(join_path(repo, 'backup'))\n",
    "os.mkdir(join_path(repo, 'results'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert annotations from xml to YOLO format. Additionally, create input txt files containing images paths, as required by YOLO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_img_path(in_dir, out_path):\n",
    "    l = glob.glob(join_path(in_dir, '*[!.txt]'))\n",
    "    with open(out_path, 'w') as w:\n",
    "        w.write('\\n'.join(l))\n",
    "        \n",
    "#converting annotations\n",
    "with cwd(repo, 'data', 'flags'):\n",
    "    for f in os.listdir(): #train, valid, test\n",
    "        if os.path.isdir(f) and f in ['train', 'valid']:\n",
    "            for xml in os.listdir(join_path(f, 'AugmentedAnnotations')):\n",
    "                convert_annot_yolo(join_path(f, 'AugmentedAnnotations', xml), detection_classes, join_path(f, 'AugmentedImages'))\n",
    "\n",
    "#creating input paths to images\n",
    "with cwd(repo):\n",
    "    collect_img_path(join_path('data','flags','train','AugmentedImages'), join_path('data','flags','train.txt'))\n",
    "    collect_img_path(join_path('data','flags','valid','AugmentedImages'), join_path('data','flags','valid.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of _cfg/flag.data_ is as follows\n",
    "```\n",
    "  classes = 1\n",
    "  train  = <path-to-data>/train.txt\n",
    "  valid  = <path-to-data>/valid.txt\n",
    "  names = <path-to-data>/flag.names\n",
    "  backup = /mydrive/yolov3\n",
    "```\n",
    "_flag.names_ should contain the list of classes, one per line.  \n",
    "\n",
    "Download weights for the classifier (Darknet-53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://pjreddie.com/media/files/darknet53.conv.74\n",
    "os.rename('darknet53.conv.74', join_path(repo,'cfg','darknet53.conv.74'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the cfg file for _yolo-v3_. Need to set adeguate `batch` (number of images per training step) and `subdivisions` (further subdivide the batch to speedup training) values according to the hardware at disposal. Then need to update `classes` in yolo layer to match the number of classes that needs to be predicted. Finally, adjust `filters` amount in convolutional layer prior to yolo layer, to match the updated number of classes, according to the formula:\n",
    "```\n",
    "filters = (classes + 5)*3\n",
    "```\n",
    "The file _yolo-v3.cfg_ provided is already setup in such a way.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/gdrive')\n",
    "! ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
    "! mkdir -p \"/mydrive/yolov3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean():\n",
    "    output.clear()\n",
    "\n",
    "#clean every minute, for 12 hours\n",
    "for x in range(20,1*60*60*12,60):\n",
    "    timer = threading.Timer(x, clean)\n",
    "    timer.start()\n",
    "\n",
    "with cwd(repo):\n",
    "    ! chmod 755 darknet\n",
    "    ! ./darknet detector train cfg/flag.data cfg/yolov3.cfg cfg/darknet53.conv.74 -dont_show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We start by importing our trained model into opencv. To predict we can use`predict_yolo()`, which takes as input the network, and image, and predicts and returns the bounding boxes. They are encoded with one bounding box per row, represented as `center_x, center_y, w, h, confidence` where `center_x, center_y` are the center pixels of the bounding box, `w` and `h` are half of the width and half of the height respectively. Finally, `confidence` stands for the prediction confidence of the network.\n",
    "\n",
    "At this stage we are interested in assessing the quality of our model out of sample, computing the mAP. To do that we use a wrapper function that takes a generic model, custom predict function for that model, class labels and image folder, and computes the metric of interest. As for the optional parameters, we are asking to compute mAP according to both pascal_voc definiton and COCO. Finally, `net_input` refers to the image input size fo YOLO, as specified in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /mydrive/yolov3/yolov3.backup /content\n",
    "os.rename('yolov3.backup', join_path(repo, 'cfg', 'volov3.weights'))\n",
    "\n",
    "with cwd(repo, 'cfg'):\n",
    "    net = cv2.dnn.readNet(\"volov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "mAP = evaluate_model(net, predict_yolo, detection_classes, mdl_type='detection', \n",
    "                     path=join_path(repo,'data','flags','test'),\n",
    "                     mAP_type='both', net_input_w=608, net_input_h=608)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8wcVJrVe2pc"
   },
   "source": [
    "### SSD Preprocessing\n",
    "Let's choose the model and the hyperparameters. For SSD we will use a `ssd_mobilenet_v2`, which offers good tradeoff between speed and accuracy and it is specifically designed to run on mobile device. Along with the model let's also set the hyper parameters associated with the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvkvBCqGeKRH"
   },
   "outputs": [],
   "source": [
    "num_steps = 40000 \n",
    "num_eval_steps = 50\n",
    "MODEL = 'ssd_mobilenet_v2_coco_2018_03_29'\n",
    "pipeline_file = 'ssd_mobilenet_v2_coco.config'\n",
    "batch_size = 12\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKlIJesNjzLp"
   },
   "source": [
    "First of all we need to clone the repository of TensorFlow model and install all the required lbraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynHCHubEgl7u"
   },
   "outputs": [],
   "source": [
    "os.mkdir(os.path.join(repo, 'OD_SSD'))\n",
    "with cwd(repo, 'OD_SSD'):\n",
    "  ! git clone --quiet https://github.com/tensorflow/models.git\n",
    "\n",
    "! pip install tf_slim\n",
    "! pip install tf-models-official\n",
    "! apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
    "! pip install -q Cython contextlib2 pillow lxml matplotlib\n",
    "! pip install -q pycocotools\n",
    "! pip install tf-models-official\n",
    "! pip install lvis\n",
    "\n",
    "with cwd(repo, 'OD_SSD','models','research'):  \n",
    "  ! protoc object_detection/protos/*.proto --python_out=.\n",
    "  os.environ['PYTHONPATH'] += ':/content/windspeed/OD_SSD/models/research/:/content/windspeed/OD_SSD/models/research/slim/'\n",
    "  ! python object_detection/builders/model_builder_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXptfVHsj_cD"
   },
   "source": [
    "In order to use TensorFlow we first need to encode our data in the `.tfrecord` format. In order to do so we first need to convert all the annotations in a single `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bU00UGhjo-j"
   },
   "outputs": [],
   "source": [
    "# create the .csv files\n",
    "for f in ['train','valid']:\n",
    "  image_path = os.path.join(repo, 'data/flags',f,'AugmentedAnnotations')\n",
    "  xml_df = xml_to_csv(image_path)\n",
    "  xml_df.to_csv(os.path.join(repo,'OD_SSD', f + '_labels.csv'), index=None)\n",
    "\n",
    "# create the .tfrecord files\n",
    "os.mkdir(os.path.join(repo,'OD_SSD','tfrecord'))\n",
    "\n",
    "with cwd(repo,'OD_SSD','tfrecord'):\n",
    "  ! wget https://raw.githubusercontent.com/fllay/totoro/master/tfrecord/generate_tfrecord.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VC6Shw0kc65"
   },
   "outputs": [],
   "source": [
    "# !!!! before running this command it is necessary to modify the generate_tfrecord.py\n",
    "# I have kept this form because I am a little bit confuse on how to put it in utils.py \n",
    "with cwd(repo, 'OD_SSD','tfrecord'):\n",
    "  ! python generate_tfrecord.py --csv_input=/content/windspeed/OD_SSD/train_labels.csv  --output_path=train.tfrecord --image_dir=/content/windspeed/data/flags/train/AugmentedImages \n",
    "  ! python generate_tfrecord.py --csv_input=/content/windspeed/OD_SSD/val_labels.csv  --output_path=val.tfrecord --image_dir=/content/windspeed/data/flags/valid/AugmentedImages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlRMEx1klIWM"
   },
   "source": [
    "We then dowload from TensorFlow model zoo a pretrained model as a starting point to train the model on our dataset in the pretrained model folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cumFBU0hlEfI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "#with cwd(repo, 'OD_SSD','models','research'):\n",
    "\n",
    "MODEL = 'ssd_mobilenet_v2_coco_2018_03_29'\n",
    "\n",
    "MODEL_FILE = MODEL + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "DEST_DIR = os.path.join (os.getcwd(), repo, 'OD_SSD/pretrained_model')\n",
    "\n",
    "if not (os.path.exists(MODEL_FILE)):\n",
    "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "\n",
    "tar = tarfile.open(MODEL_FILE)\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "os.remove(MODEL_FILE)\n",
    "if (os.path.exists(DEST_DIR)):\n",
    "    shutil.rmtree(DEST_DIR)\n",
    "os.rename(MODEL, DEST_DIR)\n",
    "\n",
    "# we the path of the pretrained model to the following variable\n",
    "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
    "\n",
    "# and create the label map, to map each the prediction with the lable\n",
    "label_map_v1('Flag')\n",
    "\n",
    "#Then we assigned to a variable of the .tfrecord and of the .pbtxt to make the notation clearer\n",
    "test_record_fname = os.path.join(os.getcwd(), repo, 'OD_SSD/tfrecord/val.tfrecord')\n",
    "train_record_fname = os.path.join(os.getcwd(), repo, 'OD_SSD/tfrecord/train.tfrecord')\n",
    "label_map_pbtxt_fname = os.path.join(os.getcwd(), repo, 'OD_SSD/label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jf6Yg-2jm1hM"
   },
   "source": [
    "Then we need to configure the pipeline using the following hyper paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lx44bL3Lmzxj"
   },
   "outputs": [],
   "source": [
    "num_steps = 40000 \n",
    "num_eval_steps = 50\n",
    "pipeline_file = 'ssd_mobilenet_v2_coco.config'\n",
    "batch_size = 12\n",
    "num_classes = 1\n",
    "\n",
    "pipeline_fname = os.path.join(os.getcwd(), repo, 'OD_SSD/models/research/object_detection/samples/configs/', pipeline_file)\n",
    "\n",
    "# by calling this function we can configure the pipeline according to the selected hyperparameter\n",
    "configuring_pipeline(pipeline_fname, train_record_fname, test_record_fname, label_map_pbtxt_fname, batch_size, num_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e_IdmtgooZg"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSVYqsP7orQj"
   },
   "outputs": [],
   "source": [
    "os.mkdir(os.path.join(repo,'OD_SSD','training_model'))\n",
    "model_dir = os.path.join(repo,'OD_SSD','training_model')\n",
    "!python /content/models/research/object_detection/model_main.py \\\n",
    "    --pipeline_config_path={pipeline_fname} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps={num_steps} \\\n",
    "    --num_eval_steps={num_eval_steps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIjXev8bo5Pf"
   },
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NJWOeGco6nv"
   },
   "outputs": [],
   "source": [
    "output_directory = os.path.join(os.getcwd(), repo, 'OD_SSD/fined_tuned_model' ) \n",
    "lst = os.listdir(model_dir)\n",
    "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
    "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
    "last_model = lst[steps.argmax()].replace('.meta', '')\n",
    "\n",
    "last_model_path = os.path.join(model_dir, last_model)\n",
    "print(last_model_path)\n",
    "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
    "    --input_type=image_tensor \\\n",
    "    --pipeline_config_path={pipeline_fname} \\\n",
    "    --output_directory={output_directory} \\\n",
    "    --trained_checkpoint_prefix={last_model_path}\n",
    "pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download(pb_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iRV-0fdpRIB"
   },
   "source": [
    "### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OnW9fdDpSgM"
   },
   "outputs": [],
   "source": [
    "sys.path.append('/content/windspeed/OD_SSD/models/research')\n",
    "\n",
    "# need to check\n",
    "import pathlib\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "PATH_TO_CKPT = os.path.join(os.getcwd(),repo, 'OD_SSD/fined_tuned_model/frozen_inference_graph.pb')\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join(os.getcwd(),repo, 'OD_SSD/label_map.pbtxt')\n",
    "\n",
    "mAP = evaluate_model(PATH_TO_CKPT, predict_fn_ssd, detection_classes, mdl_type='detection', \n",
    "                     path=join_path(repo,'data','flags','test'), PATH_TO_LABELS = PATH_TO_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CrWgTmiv4Vh"
   },
   "source": [
    "### DELETE\n",
    "This code is to visually check whether the model work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNG6XPLIv2wP"
   },
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(image, graph):\n",
    "  with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "      # Get handles to input and output tensors\n",
    "      ops = tf.get_default_graph().get_operations()\n",
    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "      tensor_dict = {}\n",
    "      for key in [\n",
    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
    "          'detection_classes', 'detection_masks'\n",
    "      ]:\n",
    "        tensor_name = key + ':0'\n",
    "        if tensor_name in all_tensor_names:\n",
    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "              tensor_name)\n",
    "      if 'detection_masks' in tensor_dict:\n",
    "        # The following processing is only for single image\n",
    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "        detection_masks_reframed = tf.cast(\n",
    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "        # Follow the convention by adding back the batch dimension\n",
    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "            detection_masks_reframed, 0)\n",
    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "      # Run inference\n",
    "      output_dict = sess.run(tensor_dict,\n",
    "                              feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "      output_dict['detection_classes'] = output_dict[\n",
    "          'detection_classes'][0].astype(np.uint8)\n",
    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "      if 'detection_masks' in output_dict:\n",
    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "  \n",
    "  return output_dict\n",
    "\n",
    "TEST_IMAGE_PATHS = ['/content/windspeed/data/flags/test/Images/100flag.jpg']\n",
    "\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  image = Image.open(image_path)\n",
    "  # the array based representation of the image will be used later in order to prepare the\n",
    "  # result image with boxes and labels on it.\n",
    "  image_np = load_image_into_numpy_array(image)\n",
    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "  # Actual detection.\n",
    "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "  # Visualization of the results of a detection.\n",
    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      output_dict['detection_boxes'],\n",
    "      output_dict['detection_classes'],\n",
    "      output_dict['detection_scores'],\n",
    "      category_index,\n",
    "      instance_masks=output_dict.get('detection_masks'),\n",
    "      use_normalized_coordinates=True,\n",
    "      line_thickness=8)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(image_np)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
